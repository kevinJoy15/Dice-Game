{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Dice Game\n",
    "## Assignment Preamble\n",
    "Please ensure you carefully read all of the details and instructions on the assignment page, this section, and the rest of the notebook. If anything is unclear at any time please post on the forum or ask a tutor well in advance of the assignment deadline.\n",
    "\n",
    "In addition to all of the instructions in the body of the assignment below, you must also follow the following technical instructions for all assignments in this unit. *Failure to do so may result in a grade of zero.*\n",
    "* [At the bottom of the page](#Submission-Test) is some code which checks you meet the submission requirements. You **must** ensure that this runs correctly before submission.\n",
    "* Do not modify or delete any of the cells that are marked as test cells, even if they appear to be empty.\n",
    "* Do not duplicate any cells in the notebook – this can break the marking script. Instead, insert a new cell (e.g. from the menu) and copy across any contents as necessary.\n",
    "\n",
    "Remember to save and backup your work regularly, and double-check you are submitting the correct version.\n",
    "\n",
    "This notebook is the primary reference for your submission. You may write code in separate `.py` files but it must be clearly imported into the notebook so that it runs without needing to reference those files, and you must explain clearly what functionality is contained in those files (through comments, markdown cells, etc).\n",
    "\n",
    "As always, **the work you submit for this assignment must be entirely your own.** Do not copy or work with other students. Do not copy answers that you find online. These assignments are designed to help improve your understanding first and foremost – the process of doing the assignment is part of *learning*. They are also used to assess your ability, and so you must uphold academic integrity. Submitting plagiarised work risks your entire place on your degree.\n",
    "\n",
    "**The pass mark for this assignment is 40%.** We expect that students, on average, will be able to produce a submission which gets a mark between 50-70%, but this will vary depending on individual backgrounds. Please ask for help if you are struggling.\n",
    "\n",
    "## Getting Started\n",
    "For this assignment, you will be writing an agent that can play a simple dice game. Here are the basic rules:\n",
    "* You start with 0 points\n",
    "* Roll three fair six-sided dice\n",
    "* Now choose one of the following:\n",
    " * Stick, accept the values shown. If two or more dice show the same values, then all of them are flipped upside down: 1 becomes 6, 2 becomes 5, 3 becomes 4, and vice versa. The total is then added to your points and this is your final score.\n",
    " * OR reroll the dice. You may choose to hold any combination of the dice on the current value shown. Rerolling costs you 1 point – so during the game and perhaps even at the end your score may be negative. You then make this same choice again.\n",
    "\n",
    "The best possible score for this game is 18 and is achieved by rolling three 1s on the first roll.\n",
    "\n",
    "The reroll penalty prevents you from rolling forever to get this score. If the value of the current dice is greater than the expected value of rerolling them (accounting for the penalty), then you should stick.\n",
    "\n",
    "The optimal decision is independent of your current score. It does not matter whether it is your first roll with a current score of 0, or your twentieth roll with a current score of -19 (in which case a positive end score is impossible), in either of these cases if you roll three 6s (which, if you stick, will only add 3 points) then you still expect to get a *better* end score by rerolling and taking the penalty. Almost any other roll will beat it, so it's still the right choice to maximise your score.\n",
    "\n",
    "It is pretty obvious that you should stick on three 1s, and reroll on three 6s. Should you hold any of the 6s when you reroll? What about other values? What should you do if the dice come up 3, 4, 5?\n",
    "\n",
    "We do not know what numbers will come up when we roll, but we do know exactly what the probability of any given roll is. This is the point of the probabilistic reasoning section of the unit; if we can model the true probabilities then we can mathematically calculate the optimal policy. Not all real world situations use dice, but these techniques work well even if we can only estimate the true probabilities.\n",
    "\n",
    "### Play The Game\n",
    "You can play the game in the following cell. Change the `SKIP_GAME` constant to `False` to enable this cell. \n",
    "<br> *Make sure to change it back to `True` before submitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SKIP_GAME = True\n",
    "if not SKIP_GAME:\n",
    "    %run dice_game.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Requirements\n",
    "The code supports playing this game with many possible modifications – you can change the number of dice, the values on the dice, or even make biased (weighted) dice that are more likely to roll certain values. More on this later.\n",
    "\n",
    "For this assignment you will need to submit:\n",
    "1. The agent implementations which answer the **4 assignments** – this notebook\n",
    " * Your code will be subject to automated testing, from which grades will be assigned depending on how well your agent plays the game (potentially with modifications)\n",
    " * To get a high grade on this assignment, the speed of your code will also be a factor – the quicker the better\n",
    " * There are some sample tests and skeleton code included below, make sure your code is compatible with the format of these tests\n",
    "2. A text file that explains your approach and the decisions you made in your own words – a readme file\n",
    " * Submissions that do not include the written section will receive zero marks – **this part is mandatory**\n",
    " * You may write your file in plain text (.txt) or [Markdown](https://www.markdownguide.org/basic-syntax/) (.md)\n",
    " * To get top marks on this assignment, as well as getting a high grade from your implementation, you must also demonstrate excellent academic presentation in your written section\n",
    " \n",
    "### Choice of Algorithm\n",
    "You can take any approach you like to write your agent. We have provided two examples for you, though these agents do not perform very well, and would not pass the assignment, they should help you structure your code.\n",
    "\n",
    "We have covered *value iteration* in the unit, and this technique will work well here to produce a strategy for the game, which your agent can then follow. It is up to you to work out the parameters that will be suit the value iteration algorithm to maximise your score.\n",
    "\n",
    "However, there are many other possible options. Simply calculating the expected value of a single roll will produce a much stronger strategy than playing randomly. You could also look up various other approaches that can be applied to Markov decision processes, such as policy iteration.\n",
    "\n",
    "To get a high grade on this assignment will require a particularly efficient implementation value iteration with intelligent choice of parameters, or something which goes beyond the material we have presented. *This is left unguided and is not factored into the unit workload estimates.*\n",
    "\n",
    "Note that you can write your agent to support modified versions of the game, which is also demonstrated below. For submissions that support this, the tests will include modified versions of the game, some where the game is *more obvious*, and some where it is *less obvious* than the default rules. Getting high marks requires supporting this feature. For those who are struggling to write a good agent, supporting this feature will result in additional tests which are more forgiving, so this might be an attractive option.\n",
    "\n",
    "If you choose to implement more than one algorithm, please feel free to include your code and write about it in part two (readme file), but only the code in this notebook will be used in the automated testing.\n",
    "\n",
    "## Dice Game Class\n",
    "A class called `DiceGame` is provided within `dice_game.py` for you to use in your solution. Here is a demonstration of its features.\n",
    "\n",
    "When you create a DiceGame object, by default you will get the rules as stated above. \n",
    "```python\n",
    "game = DiceGame()\n",
    "```\n",
    "Creates a game with 3 normal 6-sided dice. \n",
    "\n",
    "You may wish to modify the game mechanics, and you can do this using the other constructor arguments, for example:\n",
    "```python\n",
    "game = DiceGame(dice=4, sides=3, values=[1, 2, 6], bias=[0.1, 0.1, 0.8], penalty=2)\n",
    "```\n",
    "will create a game where you roll 4 dice, each with 3 sides, labelled 1, 2, and 6, where each die is far more likely to roll a 6 than they are to roll a 1 or a 2, and furthermore the penalty for rerolling is now 2 points instead of 1. *Note: this does not necessarily result in an interesting game.*\n",
    "\n",
    "In games with unusual values or sides (3-sided dice are unusual without trying to turn them upside down), when there are duplicates, `value[i]` becomes `value[-i]`. With odd-sided dice, the middle value will flip onto itself.\n",
    "\n",
    "Once created, the `DiceGame` object can be run in two different modes, *simulation* and *analysis*. It is likely that you will mostly use [*analysis* mode](#Analysis-Mode) to derive your agent's behaviour, but either way some understanding of simulation mode might be useful.\n",
    "\n",
    "### Simulation Mode\n",
    "The object provides the methods required to simulate playing the game. This might be useful if you want to test your agent, or you just want to try playing the game yourself, as we did in the cell earlier. The current dice values are found by calling `get_dice_state()`, they will always be listed in ascending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dice_game import DiceGame\n",
    "import numpy as np\n",
    "\n",
    "# setting a seed for the random number generator gives repeatable results, making testing easier!\n",
    "np.random.seed(111)\n",
    "\n",
    "game = DiceGame()\n",
    "game.get_dice_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To roll the dice, you call the `roll` method which takes one parameter: a tuple representing which dice you want to hold, numbered from zero. We rolled (2, 3, 4). Suppose we want to hold the 2, we would pass the tuple `(0,)` into the `roll` method (note we need to include the comma so that Python knows this is a tuple).\n",
    "\n",
    "The `roll` method returns a tuple containing: the reward for this action, the new state, and whether the game is over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "(2, 2, 5)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "reward, new_state, game_over = game.roll((0,))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we are happy and wish to stick to get our final score. We can call the `roll` method and supply a tuple containing all three dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(5, 5, 5)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "reward, new_state, game_over = game.roll((0, 1, 2))\n",
    "print(reward)\n",
    "print(new_state)\n",
    "print(game_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the return value is just the reward for the action, in this case 15. To get our final score we can inspect `game.score`. We rerolled once, so expect to get a score of 14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(game.score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to play again we can call `game.reset()` which returns the new starting dice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Mode\n",
    "In analysis mode, you are not playing the game, but asking the object for *all possible outcomes of certain actions*.\n",
    "\n",
    "First of all, it is useful to know that all the possible states and all the possible actions are stored inside the object. Try changing the game mechanics on the first line (e.g. add `dice=2` to the constructor) and see how the other information is updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 of 56 possible dice rolls are: [(1, 1, 1), (1, 1, 2), (1, 1, 3), (1, 1, 4), (1, 1, 5)]\n",
      "The possible actions on any given turn are: [(), (0,), (1,), (2,), (0, 1), (0, 2), (1, 2), (0, 1, 2)]\n"
     ]
    }
   ],
   "source": [
    "game = DiceGame()\n",
    "print(f\"The first 5 of {len(game.states)} possible dice rolls are: {game.states[0:5]}\")\n",
    "print(f\"The possible actions on any given turn are: {game.actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the most important method is `get_next_states(action, dice_state)`. This allows you to get all the possible resulting states for any given state and action.\n",
    "\n",
    "Earlier we had the roll of `(2, 3, 4)` and decided to hold the 2. The game can calculate all possible outcomes for us, and crucially will also give us the probability of each state occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Would get roll of (1, 1, 2) with probability 0.02777777777777778\n",
      "Would get roll of (1, 2, 2) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 3) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 4) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 5) with probability 0.055555555555555566\n",
      "Would get roll of (1, 2, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 2, 2) with probability 0.02777777777777778\n",
      "Would get roll of (2, 2, 3) with probability 0.055555555555555566\n",
      "Would get roll of (2, 2, 4) with probability 0.055555555555555566\n",
      "Would get roll of (2, 2, 5) with probability 0.055555555555555566\n",
      "Would get roll of (2, 2, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 3, 3) with probability 0.02777777777777778\n",
      "Would get roll of (2, 3, 4) with probability 0.055555555555555566\n",
      "Would get roll of (2, 3, 5) with probability 0.055555555555555566\n",
      "Would get roll of (2, 3, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 4, 4) with probability 0.02777777777777778\n",
      "Would get roll of (2, 4, 5) with probability 0.055555555555555566\n",
      "Would get roll of (2, 4, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 5, 5) with probability 0.02777777777777778\n",
      "Would get roll of (2, 5, 6) with probability 0.05555555555555559\n",
      "Would get roll of (2, 6, 6) with probability 0.027777777777777804\n"
     ]
    }
   ],
   "source": [
    "game = DiceGame()\n",
    "states, game_over, reward, probabilities = game.get_next_states((0,), (2, 3, 4))\n",
    "for state, probability in zip(states, probabilities):\n",
    "    print(f\"Would get roll of {state} with probability {probability}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method also works consistently when all dice are held, reporting that this action would cause the game to be over and giving the reward. Note that the list of states returned contains the value `None`. This is to denote that the game has entered a terminal state – no further actions would be allowed. The game does not return the final dice here, because that is another valid state (from which there would still be actions available). Also, the `reward` value is not the same as the final `score` of any given game, because it does not include any possible previous penalties for rerolling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "True\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "states, game_over, reward, probabilities = game.get_next_states((0, 1, 2), (2, 2, 5))\n",
    "print(states)\n",
    "print(game_over)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Agents to Play the Game\n",
    "\n",
    "Below this cell is where you will start writing code yourself. We will take you through multiple steps, each of which will produce a better performing agent. You can submit the strongest agent you have.\n",
    "\n",
    "Let's start with some example agents, so you can see the format we will use. In the cell below are two agents which do not play particularly well. One always holds immediately, the other will keep re-rolling all dice until they get the best possible dice (`(1, 1, 1)` or `(1, 1, 6)`), ignoring the massive penalty this will incur from re-rolling. Neither of them is considering the probabilities involved in the game.\n",
    "\n",
    "There is also a function which will run the game with an instance of a given agent. When you run the cell, it will simulate a game with each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent: \n",
      "\tAlwaysHoldAgent\n",
      "Starting dice: \n",
      "\t(1, 1, 2)\n",
      "\n",
      "Action 1: \t(0, 1, 2)\n",
      "\n",
      "Final dice: (2, 6, 6), score: 14\n",
      "\n",
      "\n",
      "Testing agent: \n",
      "\tPerfectionistAgent\n",
      "Starting dice: \n",
      "\t(2, 3, 3)\n",
      "\n",
      "Action 1: \t()\n",
      "Dice: \t\t(3, 4, 5)\n",
      "Action 2: \t()\n",
      "Dice: \t\t(1, 2, 6)\n",
      "Action 3: \t()\n",
      "Dice: \t\t(3, 4, 5)\n",
      "Action 4: \t()\n",
      "Dice: \t\t(1, 2, 5)\n",
      "Action 5: \t()\n",
      "Dice: \t\t(2, 5, 6)\n",
      "Action 6: \t()\n",
      "Dice: \t\t(1, 6, 6)\n",
      "Action 7: \t()\n",
      "Dice: \t\t(1, 2, 6)\n",
      "Action 8: \t()\n",
      "Dice: \t\t(1, 3, 6)\n",
      "Action 9: \t()\n",
      "Dice: \t\t(2, 4, 5)\n",
      "Action 10: \t()\n",
      "Dice: \t\t(1, 5, 6)\n",
      "Action 11: \t()\n",
      "Dice: \t\t(5, 5, 6)\n",
      "Action 12: \t()\n",
      "Dice: \t\t(1, 2, 5)\n",
      "Action 13: \t()\n",
      "Dice: \t\t(2, 3, 6)\n",
      "Action 14: \t()\n",
      "Dice: \t\t(1, 1, 2)\n",
      "Action 15: \t()\n",
      "Dice: \t\t(2, 2, 5)\n",
      "Action 16: \t()\n",
      "Dice: \t\t(1, 3, 4)\n",
      "Action 17: \t()\n",
      "Dice: \t\t(1, 4, 5)\n",
      "Action 18: \t()\n",
      "Dice: \t\t(1, 3, 5)\n",
      "Action 19: \t()\n",
      "Dice: \t\t(1, 3, 4)\n",
      "Action 20: \t()\n",
      "Dice: \t\t(4, 4, 6)\n",
      "Action 21: \t()\n",
      "Dice: \t\t(1, 4, 6)\n",
      "Action 22: \t()\n",
      "Dice: \t\t(1, 3, 5)\n",
      "Action 23: \t()\n",
      "Dice: \t\t(1, 3, 6)\n",
      "Action 24: \t()\n",
      "Dice: \t\t(5, 5, 6)\n",
      "Action 25: \t()\n",
      "Dice: \t\t(3, 4, 5)\n",
      "Action 26: \t()\n",
      "Dice: \t\t(2, 3, 6)\n",
      "Action 27: \t()\n",
      "Dice: \t\t(4, 4, 6)\n",
      "Action 28: \t()\n",
      "Dice: \t\t(1, 3, 6)\n",
      "Action 29: \t()\n",
      "Dice: \t\t(2, 3, 4)\n",
      "Action 30: \t()\n",
      "Dice: \t\t(1, 4, 6)\n",
      "Action 31: \t()\n",
      "Dice: \t\t(2, 4, 4)\n",
      "Action 32: \t()\n",
      "Dice: \t\t(3, 6, 6)\n",
      "Action 33: \t()\n",
      "Dice: \t\t(1, 4, 6)\n",
      "Action 34: \t()\n",
      "Dice: \t\t(2, 5, 6)\n",
      "Action 35: \t()\n",
      "Dice: \t\t(1, 5, 6)\n",
      "Action 36: \t()\n",
      "Dice: \t\t(1, 5, 5)\n",
      "Action 37: \t()\n",
      "Dice: \t\t(1, 5, 6)\n",
      "Action 38: \t()\n",
      "Dice: \t\t(1, 1, 1)\n",
      "Action 39: \t(0, 1, 2)\n",
      "\n",
      "Final dice: (6, 6, 6), score: -20\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class DiceGameAgent(ABC):\n",
    "    def __init__(self, game):\n",
    "        self.game = game\n",
    "    \n",
    "    @abstractmethod\n",
    "    def play(self, state):\n",
    "        pass\n",
    "\n",
    "\n",
    "class AlwaysHoldAgent(DiceGameAgent):\n",
    "    def play(self, state):\n",
    "        return (0, 1, 2)\n",
    "\n",
    "\n",
    "class PerfectionistAgent(DiceGameAgent):\n",
    "    def play(self, state):\n",
    "        if state == (1, 1, 1) or state == (1, 1, 6):\n",
    "            return (0, 1, 2)\n",
    "        else:\n",
    "            return ()\n",
    "        \n",
    "        \n",
    "def play_game_with_agent(agent, game, verbose=False):\n",
    "    state = game.reset()\n",
    "    \n",
    "    if(verbose): print(f\"Testing agent: \\n\\t{type(agent).__name__}\")\n",
    "    if(verbose): print(f\"Starting dice: \\n\\t{state}\\n\")\n",
    "    \n",
    "    game_over = False\n",
    "    actions = 0\n",
    "    while not game_over:\n",
    "        action = agent.play(state)\n",
    "        actions += 1\n",
    "        \n",
    "        if(verbose): print(f\"Action {actions}: \\t{action}\")\n",
    "        _, state, game_over = game.roll(action)\n",
    "        if(verbose and not game_over): print(f\"Dice: \\t\\t{state}\")\n",
    "\n",
    "    if(verbose): print(f\"\\nFinal dice: {state}, score: {game.score}\")\n",
    "        \n",
    "    return game.score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # random seed makes the results deterministic\n",
    "    # change the number to see different results\n",
    "    # or delete the line to make it change each time it is run\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    game = DiceGame()\n",
    "    \n",
    "    agent1 = AlwaysHoldAgent(game)\n",
    "    play_game_with_agent(agent1, game, verbose=True)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    \n",
    "    agent2 = PerfectionistAgent(game)\n",
    "    play_game_with_agent(agent2, game, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Hybrid Agent\n",
    "\n",
    "Up to this point, we have covered how the dice game works and provided some examples of simple agents that can play the game - specifically an agent that always holds regardless of the value on the dice, the `AlwaysHoldAgent`, and an agent that will continue to re-roll the dice until they get the perfect score, called the `PerfectionistAgent`.\n",
    "\n",
    "Obviously, as the game progresses the reward available reduces as taking more re-rolls will eventually result in so many penalties that they will always outweigh any positive score obtained. So although the perfectionist may get lucky and get a high score early in the game if the agent takes too many goes then its score will be worse than had it just held onto what it already had. As such a hybrid strategy of the `AlwaysHoldAgent` and the `PerfectionistAgent` should on average outperform them individually given enough games.\n",
    "\n",
    "Therefore your first task will be to create a hybrid agent of the `AlwaysHold` and `Perfectionist` agents. One thing you will need to investigate is when it is optimal to switch between the two strategies. When is better to chase perfection, and when is it best to just cut your losses and hold what you have?\n",
    "\n",
    "We have written a function in the dice_game.py file to help you do this: get_dice_score(), which can be used to calculate the current score on the dice, not including any penalties for rerolling.\n",
    "\n",
    "## Aims:\n",
    "\n",
    "- Complete the function provided creating a hybrid of the `AlwaysHold` and `Perfectionist` agents\n",
    "- Optimise the agent so that it outperforms both the `AlwaysHold` and `Perfectionist` agents given enough games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridAgent(DiceGameAgent):\n",
    "    def play(self, state):\n",
    "        current_score = self.game.get_dice_score()\n",
    "        \n",
    "        # Your Code Here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Manual Agent\n",
    "\n",
    "So as you may have discovered even the best hyrid agent doesn't perform that well. You may have also found that you were able to obtain a better score playing the game yourself using your intuition.\n",
    "\n",
    "So for the next assignment, your job will be to implement an agent that can outperform the `Hybrid`, `Perfectionist` and the `AlwaysHold` agents. Enter your code inside the play function below. You can use the init function to set properties if you would like, as follows:\n",
    "\n",
    "```\n",
    "def __init__(self, game):\n",
    "    super().__init__(game)\n",
    "    self.some_property = 10\n",
    "```\n",
    "\n",
    "Then it can be used inside the play function like so:\n",
    "\n",
    "```\n",
    "def play(self, state):\n",
    "    a = self.some_property\n",
    "```\n",
    "\n",
    "## Aims:\n",
    "- Implement an agent that plays how you might play.\n",
    "- This agent should be able to outperform the three agents you have seen so far - the `Hybrid`, `Perfectionist` and the `AlwaysHold` agents.\n",
    "\n",
    "\n",
    "**HINT** - This doesn't need to play perfectly, only a small number of modifications will be required to improve on the previous agents. Try not to overthink it, it is only expected that you find small improvements over the Hybrid Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManualAgent(DiceGameAgent):\n",
    "    \n",
    "    def __init__(self, game):\n",
    "        super().__init__(game)\n",
    "    \n",
    "    def play(self, state):\n",
    "        # Your Code Here.\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "Up to this point, we have focused on implementing manually created strategies such as always holding regardless of the dice values. However, the approaches taken up to this point are sub-optimal and although it may be possible to hand-craft the optimal policy, it would be very difficult, and even impossible if the problem was more complex. As such there are ways of autonomously learning the optimal policy, some of which we have covered on the course.\n",
    "\n",
    "Specifically, for this coursework, we will be focusing on **value iteration**, which is a method for learning the expected future discounted reward for every state in a game. Using this expected future reward it is possible to derive the optimal policy to take by selecting the actions that move the agent to the state with the highest expected future reward.\n",
    "\n",
    "![Value Iteration Algorithm](ValueIterationAlgorithm.png)\n",
    "\n",
    "The value iteration algorithm is provided here for your convinence and it is the algorithm that you will have to implment over the following assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - One Step Value Iteration\n",
    "\n",
    "For assignment 3 the goal will be to implement one step value iteration. The only difference between this and full value iteration is that values are only updated once, rather than continually updating them until they are within some threshold. We will be extending the work here for the final assignment to implement the full algorithm, however, for this assignment, we will only implement the modified algorithm provided here:\n",
    "\n",
    "![Value Iteration Algorithm](OneStepValueIterationAlgorithm.png)\n",
    "\n",
    "For your agent you should use the following parameter values (these are already set in the init function so please don't change them):\n",
    "\n",
    "- Gamma: 1.0\n",
    "\n",
    "\n",
    "## Aims\n",
    "- Complete the `init` and `perform_single_value_iteration` functions to perform a single step of value iteration.\n",
    "- Complete the `play` function that takes the optimal action for a given state according to the values learned by the previous functions.\n",
    "\n",
    "NOTE. One step value iteration should call the `perform_single_value_iteration` twice, as the first call will not actually step the agent, it will only initialize the values, so the second call will perform the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStepValueIterationAgent(DiceGameAgent):\n",
    "    def __init__(self, game):\n",
    "        \"\"\"\n",
    "        If your code does any pre-processing on the game, you can do it here.\n",
    "        \n",
    "        You can always access the game with self.game\n",
    "        \"\"\"\n",
    "        super().__init__(game)\n",
    "        self.gamma = 1.0\n",
    "        \n",
    "        # Your Code Here\n",
    "        \n",
    "        _, self.values = self.perform_single_value_iteration()\n",
    "        _, self.values = self.perform_single_value_iteration()\n",
    "    \n",
    "    def perform_single_value_iteration(self):\n",
    "        # Your Code Here\n",
    "        \n",
    "        return delta, new_values\n",
    "    \n",
    "    def play(self, state):\n",
    "        \"\"\"\n",
    "        given a state, return the chosen action for this state\n",
    "        at minimum you must support the basic rules: three six-sided fair dice\n",
    "        \n",
    "        if you want to support more rules, use the values inside self.game, e.g.\n",
    "            the input state will be one of self.game.states\n",
    "            you must return one of self.game.actions\n",
    "        \n",
    "        read the code in dicegame.py to learn more\n",
    "        \"\"\"\n",
    "        # Your Code Here\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Full Value Iteration\n",
    "\n",
    "Having completed a single step value iteration you can finsih the full algorithm shown here:\n",
    "\n",
    "![Value Iteration Algorithm](ValueIterationAlgorithm.png)\n",
    "\n",
    "There are 4 functions to complete as a part of this exercise, you have already done 3 of them so you are more than welcome to copy their contents and re-use them again here. In addition, you must complete the `value_iteration` that will call the `perform_single_value_iteration` function until the threshold is met. This will also require some small changes to the `init` function. Finally, you must investigate what are the optimal parameters for the algorithm in order to get the best results. **This part is very important!**\n",
    "\n",
    "## Aims\n",
    "\n",
    "- Correctly implement full value iteration\n",
    "- Optimise the parameters (gamma, theta) to get the best agent possible\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class ValueIterationAgent(DiceGameAgent):\n",
    "    def __init__(self, game):\n",
    "        \"\"\"\n",
    "        If your code does any pre-processing on the game, you can do it here\n",
    "        \n",
    "        You can always access the game with self.game\n",
    "        \"\"\"\n",
    "        super().__init__(game)\n",
    "        \n",
    "        # These are default values, you can play around with these to improve your agents performance.\n",
    "        self.theta = 0.01\n",
    "        self.gamma = 0.9\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    \n",
    "    def perform_single_value_iteration(self):\n",
    "        # We advice you use the function from the previous assignment. You do not have to if you don't want to but it should save you alot of time.\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return delta, new_values\n",
    "    \n",
    "    def value_iteration(self, theta=0.001):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def play(self, state):\n",
    "        \"\"\"\n",
    "        given a state, return the chosen action for this state\n",
    "        at minimum you must support the basic rules: three six-sided fair dice\n",
    "        \n",
    "        if you want to support more rules, use the values inside self.game, e.g.\n",
    "            the input state will be one of self.game.states\n",
    "            you must return one of self.game.actions\n",
    "        \n",
    "        read the code in dicegame.py to learn more\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Point\n",
    "\n",
    "Below, you should choose your best performing agent to submit for this coursework. You can simply copy and paste the code you want to use from a previous task. The following is a guide to the mark you should expect for a correct implementation of each step:\n",
    "\n",
    "- Hybrid Agent: 40-50 marks.\n",
    "- Manual Agent: 40-60 marks.\n",
    "- One Step Value Iteration: 60-70 marks\n",
    "- Non-optimal value Iteration: 70-80 marks\n",
    "- Optimal Value Iteration + Extended Rules Compatability: 80+ marks\n",
    "\n",
    "Please note that these are only rough guidelines: your actual mark will depend on the performance of your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e008ab09924c20788e70f0d8c550096f",
     "grade": false,
     "grade_id": "cell-bf3ca4a1f7dfcf30",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_score(dice, state):                  # this function returns the score of the next state\n",
    "    if len(state) == len(set(state)):\n",
    "        return sum(state)      # returns the score of the next value\n",
    "    else:\n",
    "        i_list = []\n",
    "        lst = list(state)\n",
    "        for a in range(0,len(lst)):\n",
    "            for b in range(a+1, len(lst)):\n",
    "                if lst[a] == lst[b]:\n",
    "                    i_list.append(a)\n",
    "                    i_list.append(b)\n",
    "        i_list = list(set(i_list))\n",
    "        \n",
    "        for index in i_list:\n",
    "                swap = -dice.game._values[list(dice.game._values).index(lst[index])]\n",
    "                lst[index] = dice.game._values[swap]\n",
    "        return sum(lst)       # returns the score of the next value\n",
    "\n",
    "\n",
    "class ValueIterationAgent(DiceGameAgent):\n",
    "\n",
    "    def __init__(self, game):\n",
    "        super().__init__(game)\n",
    "        self.theta = 0.1         # initializing the value of theta \n",
    "        self.gamma = 0.96           # initializing the value of gamma \n",
    "        # the values of theta and gamma were found through hit and trial\n",
    "        self.v = self.value_iteration()\n",
    "\n",
    "    def value_iteration(self):\n",
    "        game  = DiceGame()\n",
    "\n",
    "        V_opt = {}              # stores the V_opt values\n",
    "        m_action = 0            # stores the max action\n",
    "        policy = {}              # stores the policy for each state\n",
    "        V_sum = 0                # stores the sum of V_opt\n",
    "        delta = 0              #  V_opt(s)-V_opt(s')\n",
    "        state_val = {}          # value of the state\n",
    "        delta_max = self.theta+1  # used as a buffer marker\n",
    "\n",
    "        for s in game.states:     \n",
    "            policy[s] = ()      \n",
    "            V_opt[s] = 0        # assigning the values of V_opt to 0 for 1st iteration \n",
    "\n",
    "        while (delta_max >= self.theta):    \n",
    "            delta_max = 0\n",
    "\n",
    "            for state in game.states:      # gets all the states \n",
    "                state_val = V_opt[state]        # state_val takes a temporary value of V_opt[]\n",
    "                m_action = 0\n",
    "\n",
    "                for action in game.actions:\n",
    "                    V_sum = 0                      \n",
    "                    states, game_over, reward, probabilities = game.get_next_states(action, state)  \n",
    "                    # these line was copied from the dicegame file, which essentially rus through each action and gets its values for each individual state, reward, game_over status and transition probabilities\n",
    "\n",
    "                    for s, p in zip(states, probabilities): # cycles through each states probablity \n",
    "\n",
    "                        if game_over == False:\n",
    "\n",
    "                            V_sum = V_sum + p*(calculate_score(self, s) - calculate_score(self, state) + reward+ self.gamma * V_opt[s])   # gets the sum of all V_opts for all the states of each action\n",
    "                            #instead of using a fixed reward value we are subtracting the probablity of the next value from the previous value\n",
    "                        else:\n",
    "                            V_sum = reward - 10.5 - 3\n",
    "\n",
    "                    if V_sum > m_action:       # this checks is the new value of sum is greater than the previous value of m_action\n",
    "                        m_action = V_sum    # if so m_action gets updated\n",
    "                        policy[state] = action\n",
    "\n",
    "                delta = abs(state_val - V_opt[state])  # calculates the delta between the 2 V_opts\n",
    "                if(delta > delta_max):\n",
    "                    delta_max = delta      # delta max is the final difference that is taken use of for in the while statement\n",
    "\n",
    "                V_opt[state] = m_action\n",
    "        return policy       # returns a policy\n",
    "\n",
    "    def play(self, state):\n",
    "        return self.v[state]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent: \n",
      "\tValueIterationAgent\n",
      "Starting dice: \n",
      "\t(2, 3, 3)\n",
      "\n",
      "Action 1: \t(1, 2)\n",
      "Dice: \t\t(3, 3, 4)\n",
      "Action 2: \t()\n",
      "Dice: \t\t(2, 3, 5)\n",
      "Action 3: \t(0,)\n",
      "Dice: \t\t(1, 2, 6)\n",
      "Action 4: \t(0,)\n",
      "Dice: \t\t(1, 3, 5)\n",
      "Action 5: \t(0,)\n",
      "Dice: \t\t(1, 1, 4)\n",
      "Action 6: \t(0, 1, 2)\n",
      "\n",
      "Final dice: (4, 6, 6), score: 11\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # random seed makes the results deterministic\n",
    "    # change the number to see different results\n",
    "    # or delete the line to make it change each time it is run\n",
    "    np.random.seed(1)\n",
    "    \n",
    "    game = DiceGame()\n",
    "    \n",
    "    agent1 = ValueIterationAgent(game)\n",
    "    play_game_with_agent(agent1, game, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opt-In For Extended Rules\n",
    "\n",
    "If you wish your code to be tested with rules other than the defaults, set the constant `TEST_EXTENDED_RULES` to `True` on the next line. Another test will be performed to check your code still works. If you get an error, you are likely not supporting the extended rules properly.\n",
    "\n",
    "Refer to [this section](#Choice-of-Algorithm) to understand more about how extended rules factor into your possible grade.\n",
    "\n",
    "**Note:** you need to have `SKIP_TESTS` set to `False` in the cell above (and run it!) to enable the tests below. The value of `TEST_EXTENDED_RULES` *will* be used in the grading tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing extended rules – two three-sided dice.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MyAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-ba3270c784f0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mSKIP_TESTS\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mTEST_EXTENDED_RULES\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mextended_tests\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-ba3270c784f0>\u001b[0m in \u001b[0;36mextended_tests\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtest_agent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mtotal_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MyAgent' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "SKIP_TESTS = False\n",
    "TEST_EXTENDED_RULES = True\n",
    "\n",
    "def extended_tests():\n",
    "    import time \n",
    "    total_score = 0\n",
    "    total_time = 0\n",
    "    n = 10\n",
    "\n",
    "    print(\"Testing extended rules – two three-sided dice.\")\n",
    "    print()\n",
    "\n",
    "    game = DiceGame(dice=2, sides=3)\n",
    "\n",
    "        \n",
    "    start_time = time.process_time()\n",
    "    test_agent = MyAgent(game)\n",
    "    total_time += time.process_time() - start_time\n",
    "\n",
    "    for i in range(n):\n",
    "        start_time = time.process_time()\n",
    "        score = play_game_with_agent(test_agent, game)\n",
    "        total_time += time.process_time() - start_time\n",
    "\n",
    "        print(f\"Game {i} score: {score}\")\n",
    "        total_score += score\n",
    "\n",
    "    print()\n",
    "    print(f\"Average score: {total_score/n}\")\n",
    "    print(f\"Average time: {total_time/n:.5f} seconds\")\n",
    "\n",
    "    \n",
    "\n",
    "if not SKIP_TESTS and TEST_EXTENDED_RULES:\n",
    "    extended_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Test\n",
    "The following cell tests if your notebook is ready for submission. **You must not skip this step!**\n",
    "\n",
    "Restart the kernel and run the entire notebook (Kernel → Restart & Run All). Now look at the output of the cell below. \n",
    "\n",
    "*If there is no output, then your submission is not ready.* Either your code is still running (did you forget to skip tests?) or it caused an error.\n",
    "\n",
    "As previously mentioned, failing to follow these instructions can result in a grade of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "\n",
    "fail = False;\n",
    "\n",
    "if not SKIP_GAME:\n",
    "    fail = True;\n",
    "    print(\"You must set the SKIP_GAME constant to True in the earlier cell.\")\n",
    "\n",
    "if not SKIP_TESTS:\n",
    "    fail = True;\n",
    "    print(\"You must set the SKIP_TESTS constant to True in the earlier cell.\")\n",
    "\n",
    "\n",
    "p3 = pathlib.Path('./dicegame.ipynb')\n",
    "if not p3.is_file():\n",
    "    fail = True\n",
    "    print(\"This notebook file must be named dicegame.ipynb\")\n",
    "    \n",
    "if \"MyAgent\" not in dir():\n",
    "    fail = True;\n",
    "    print(\"You must include a class called MyAgent as defined above.\")\n",
    "else:    \n",
    "    game = DiceGame()\n",
    "    agent = MyAgent(game)\n",
    "    action = agent.play((1, 1, 1))\n",
    "\n",
    "    if action not in game.actions:\n",
    "        print(\"Warning:\")\n",
    "        print(\"Your agent does not seem to produce a valid action with the default rules.\")\n",
    "        print()\n",
    "        print(\"Your assignment is unlikely to get any marks from the autograder. While we will\")\n",
    "        print(\"try to check it manually to assign some partial credit, we encourage you to ask\")\n",
    "        print(\"for help on the forum or directly to a tutor.\")\n",
    "        print()\n",
    "        print(\"Please use the readme file to explain your code anyway.\")\n",
    "        \n",
    "    if TEST_EXTENDED_RULES:\n",
    "        print(\"Info: TEST_EXTENDED_RULES is ON\")\n",
    "        game = DiceGame(dice=2, sides=8)\n",
    "        agent = MyAgent(game)\n",
    "        try:\n",
    "            action = agent.play((7, 8))\n",
    "        except:\n",
    "            action = None\n",
    "\n",
    "        if action not in game.actions:\n",
    "            fail = True\n",
    "            print(\"Your agent does not produce a valid action with the extended rules.\")\n",
    "            print(\"Turn off TEST_EXTENDED_RULES if you cannot fix this error.\")\n",
    "    else:\n",
    "        print(\"Info: TEST_EXTENDED_RULES is OFF (extended rules will not be tested)\")\n",
    "    \n",
    "if fail:\n",
    "    print()\n",
    "    sys.stderr.write(\"Your submission is not ready! Please read and follow the instructions above.\")\n",
    "else:\n",
    "    print()\n",
    "    print(\"All checks passed. When you are ready to submit, upload the notebook and readme file to the\")\n",
    "    print(\"assignment page, without changing any filenames.\")\n",
    "    print()\n",
    "    print(\"If you need to submit multiple files, you can archive them in a .zip file. (No other format.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55b9f0ac60c35905eafd9d1d90fe2e85",
     "grade": true,
     "grade_id": "cell-6cb9d7e6d9c7b590",
     "locked": true,
     "points": 100,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Auto-Marked cell, DO NOT DELTE, MODIFY OR DUPLICATE!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
